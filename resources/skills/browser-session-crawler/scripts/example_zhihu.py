"""
çŸ¥ä¹çˆ¬å–ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ browser-session-crawler çˆ¬å–éœ€è¦ç™»å½•çš„å†…å®¹
"""

import asyncio
import json
from playwright.async_api import Page
import sys
import os

# æ·»åŠ è„šæœ¬ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from crawl import crawl_with_session, get_default_chrome_user_data_dir


async def crawl_zhihu_feed(page: Page):
    """çˆ¬å–çŸ¥ä¹é¦–é¡µåŠ¨æ€"""
    
    # ç­‰å¾…å†…å®¹åŠ è½½
    await page.wait_for_selector(".ContentItem", timeout=15000)
    
    # è·å–å†…å®¹é¡¹
    items = await page.query_selector_all(".ContentItem")
    
    results = []
    for item in items[:20]:  # å–å‰20æ¡
        try:
            # æ ‡é¢˜
            title_elem = await item.query_selector(".ContentItem-title")
            title = await title_elem.inner_text() if title_elem else ""
            
            # æ‘˜è¦
            content_elem = await item.query_selector(".RichContent-inner")
            content = await content_elem.inner_text() if content_elem else ""
            
            # ä½œè€…
            author_elem = await item.query_selector(".AuthorInfo-name")
            author = await author_elem.inner_text() if author_elem else "åŒ¿å"
            
            if title:
                results.append({
                    "title": title.strip(),
                    "content": content.strip()[:200] + "..." if len(content) > 200 else content.strip(),
                    "author": author.strip()
                })
        except Exception as e:
            continue
    
    return results


async def main():
    print("="*50)
    print("ğŸ§ª çŸ¥ä¹çˆ¬å–ç¤ºä¾‹")
    print("="*50)
    
    # æ£€æŸ¥æµè§ˆå™¨
    user_data_dir = get_default_chrome_user_data_dir()
    if user_data_dir:
        print(f"âœ… æ‰¾åˆ°æµè§ˆå™¨æ•°æ®: {user_data_dir}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°æµè§ˆå™¨æ•°æ®")
    
    # æ‰§è¡Œçˆ¬å–
    result = await crawl_with_session(
        target_url="https://www.zhihu.com/",
        logged_in_indicator=".AppHeader-profile",  # çŸ¥ä¹ç™»å½•åå‡ºç°çš„å…ƒç´ 
        crawl_function=crawl_zhihu_feed,
        # login_url="https://www.zhihu.com/signin",  # å¯é€‰ï¼šç™»å½•é¡µ
    )
    
    # è¾“å‡ºç»“æ„åŒ– JSON ä¾›è°ƒç”¨æ–¹è§£æ
    output = {
        "source": "zhihu",
        "items": result,
        "count": len(result)
    }
    print(json.dumps(output, ensure_ascii=False))

    return result


if __name__ == "__main__":
    asyncio.run(main())
